{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/splits/train/stock_data.json', 'r') as inp:\n",
    "    train_stock = json.load(inp)\n",
    "with open('../../data/processed/splits/valid/stock_data.json', 'r') as inp:\n",
    "    valid_stock = json.load(inp)\n",
    "with open('../../data/processed/splits/test/stock_data.json', 'r') as inp:\n",
    "    test_stock = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = Field(\n",
    "    sequential=False\n",
    ")\n",
    "TRANSCRIPT = Field(\n",
    "    sequential=True,\n",
    "    fix_length=11000,\n",
    "    lower=True\n",
    ")\n",
    "LABEL = Field(\n",
    "    sequential=False,\n",
    "    dtype=torch.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(data.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        examples = np.array(examples)\n",
    "        self.labels = examples[:,-1]\n",
    "        self.market_cap = examples[:,-2]\n",
    "        self.examples = np.array(examples[:,:-2].tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = torch.tensor(self.examples[index])\n",
    "        auxiliary = self.market_cap[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, auxiliary, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets():\n",
    "    train, valid, test = TabularDataset.splits(\n",
    "        path='../../data/processed/splits',\n",
    "        format='csv',\n",
    "        skip_header=True,\n",
    "        train='train/transcripts.csv',\n",
    "        validation='valid/transcripts.csv',\n",
    "        test='test/transcripts.csv',\n",
    "        fields=[('id', ID), ('transcript', TRANSCRIPT), ('post_high', LABEL)]\n",
    "    )\n",
    "    glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "    TRANSCRIPT.build_vocab(train, valid, test, vectors=glove)\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingGlassDataset(data.Dataset):\n",
    "    def __init__(self, stock_dataset, transcript_dataset):\n",
    "        self.stock_data = stock_dataset\n",
    "        self.transcript_data = transcript_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.stock_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        stocks, stocks_aux, label = self.stock_data[index]\n",
    "        transcript_example = self.transcript_data[index]\n",
    "        assert math.isclose(float(transcript_example.post_high), label)\n",
    "\n",
    "        transcript = torch.tensor(transcript_example.transcript)\n",
    "        return stocks, stocks_aux, transcript, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as inp:\n",
    "    VOCAB = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transcripts(train, valid, test):\n",
    "    for dataset in [train, valid, test]:\n",
    "        for example in dataset:\n",
    "            if len(example.transcript) > 11000:\n",
    "                example.transcript = example.transcript[:11000]\n",
    "            else:\n",
    "                remainder = 11000 - len(example.transcript)\n",
    "                example.transcript += ['<pad>']*remainder\n",
    "            example.transcript = list(map(lambda x: VOCAB.stoi[x], example.transcript))\n",
    "    return train, valid, test\n",
    "\n",
    "train_transcript, valid_transcript, test_transcript = preprocess_transcripts(*build_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_datasets = {\n",
    "    'train': StockDataset(train_stock),\n",
    "    'valid': StockDataset(valid_stock),\n",
    "    'test': StockDataset(test_stock)\n",
    "}\n",
    "\n",
    "train_dataset = LookingGlassDataset(stock_datasets['train'], train_transcript)\n",
    "valid_dataset = LookingGlassDataset(stock_datasets['valid'], valid_transcript)\n",
    "test_dataset = LookingGlassDataset(stock_datasets['test'], test_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.pkl', 'wb') as out:\n",
    "#     pickle.dump(TRANSCRIPT.vocab, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineStockPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that will read in plain stock ticker values over time and decide whether to buy, sell, or hold at the current price.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_series_features=1, num_auxiliary_features=1, hidden_size=128, output_size=1):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            num_series_features: the size of the feature set for an individual\n",
    "                                 stock price example (e.g. if we include high,\n",
    "                                 low, average, num_series_features will equal 3\n",
    "            num_auxiliary_features: the number of auxiliary (not dependent on time)\n",
    "                                    features we are adding (e.g. if we include the 1yr\n",
    "                                    high and the market capitalization, num_auxiliary_features\n",
    "                                    would equal 2\n",
    "            output_size: the size of the outputted vector. For evaluation, we would use a\n",
    "                         size of 1 (stock price) or 3 (buy, sell, hold classification).\n",
    "                         For use in the looking glass model, we want an encoding so we might\n",
    "                         use a size of 128 to feed into the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.recurrent = nn.LSTM(\n",
    "            input_size=num_series_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        # concatenate LSTM output with auxiliary features\n",
    "        # output predicted price\n",
    "        self.linear = nn.Linear(hidden_size+num_auxiliary_features, output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model\n",
    "        \"\"\"\n",
    "        for layer in [self.linear]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, X_series, X_auxiliary):\n",
    "        \"\"\"\n",
    "        Moves the model through each layer\n",
    "        Parameters:\n",
    "            X_series: an [N, num_series_examples, num_series_features] size vector\n",
    "                      where N is the batch size, num_series_examples is how many stock prices\n",
    "                      we are providing per example (e.g. weekly for the last 3 months), and\n",
    "                      num_series_features is the same as described in __init__\n",
    "            X_auxiliary: an [N, num_auxiliary_features] vector\n",
    "        \"\"\"\n",
    "        recurrent_output,_ = self.recurrent(X_series)\n",
    "        recurrent_output = torch.mean(recurrent_output, 1)\n",
    "        # We might need this\n",
    "        # recurrent_output = torch.squeeze(1) \n",
    "        aux_combined = torch.cat([recurrent_output, X_auxiliary], dim=1)\n",
    "        output = self.linear(aux_combined)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingGlassPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that will use the Baseline predictor as well as earnings call information to decide whether to buy, sell, or hold at the current price\n",
    "    \"\"\"\n",
    "    def __init__(self, num_series_features=1, hidden_size=64, num_auxiliary_features=1, max_call_len=11000):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        Attributes:\n",
    "            (see baseline.py for num_series_features and num_auxiliary_features)\n",
    "            max_call_len: maximum number of tokens allowed in an earnings call transcript.\n",
    "                          We will need to pad each earnings call to be this length (or truncate\n",
    "                          if the call is too long)\n",
    "            num_auxiliary_call_features: # non-transcript related features (e.g. if we\n",
    "                                         include sentiment, ambiguity score, and\n",
    "                                         confidence score, the num_auxiliary_call_features\n",
    "                                         would equal 3\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.baseline = BaselineStockPredictor(\n",
    "            num_series_features=num_series_features,\n",
    "            num_auxiliary_features=num_auxiliary_features,\n",
    "            output_size=hidden_size\n",
    "        ).cuda()\n",
    "        self.embedding = nn.Embedding.from_pretrained(VOCAB.vectors, freeze=True)\n",
    "        self.recurrent = nn.LSTM(\n",
    "            input_size=50,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.rec_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.combined_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.final_linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the model weights\n",
    "        \"\"\"\n",
    "        self.baseline.init_weights()\n",
    "        for layer in [self.rec_linear, self.combined_linear, self.final_linear]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, X_series, X_auxiliary, X_transcript):\n",
    "        \"\"\"\n",
    "        Moves the model through each layer\n",
    "        Parameters:\n",
    "            (see baseline.py for X_series and X_auxiliary)\n",
    "            X_transcript: an [N, max_series_features, embedding_size] vector\n",
    "            X_transcript_auxiliary: an [N, num_auxiliary_features] vector\n",
    "        \"\"\"\n",
    "        baseline_output = self.baseline.forward(X_series, X_auxiliary)\n",
    "        baseline_activated = nn.functional.relu(baseline_output)\n",
    "\n",
    "        transcript_embeddings = self.embedding(X_transcript)\n",
    "        recurrent_output,_ = self.recurrent(transcript_embeddings)\n",
    "        recurrent_output = torch.mean(recurrent_output, 1)\n",
    "        \n",
    "#         aux_combined = torch.cat([recurrent_output, X_transcript_auxiliary], dim=1)\n",
    "        output = self.rec_linear(recurrent_output)\n",
    "        output_activated = nn.functional.relu(output)\n",
    "\n",
    "        stock_transcript_joint_layer = torch.cat([baseline_activated, output_activated], dim=1)\n",
    "        z1 = self.combined_linear(stock_transcript_joint_layer)\n",
    "        a1 = nn.functional.relu(z1)\n",
    "        \n",
    "        final_output = self.final_linear(a1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lg_iterator(dataset, batch_size, train=True, shuffle=True):\n",
    "    iterator = data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=5)\n",
    "    return iterator\n",
    "    \n",
    "def train_model(train, valid, num_epochs=200, learning_rate=0.003):\n",
    "    batch_size = 64\n",
    "    train_iterator = get_lg_iterator(train, batch_size)\n",
    "    valid_iterator = get_lg_iterator(valid, batch_size)\n",
    "    \n",
    "    model = LookingGlassPredictor(num_series_features=2, hidden_size=64)\n",
    "    model = model.float()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    valid_scores = []\n",
    "    \n",
    "    min_mse = float('inf')\n",
    "    delay = 0\n",
    "    MAX_INC = 100\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(num_epochs)):\n",
    "        model.train()\n",
    "        iter_losses = []\n",
    "        print('Starting epoch', epoch)\n",
    "        for batch_stock_series, batch_stock_aux, batch_transcripts, batch_labels in train_iterator:\n",
    "            batch_stock_aux = torch.reshape(batch_stock_aux, (-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_stock_series.float().cuda(), batch_stock_aux.float().cuda(), batch_transcripts.long().cuda())\n",
    "            batch_labels = torch.reshape(batch_labels, (-1,1))\n",
    "            loss = criterion(outputs.cuda(), batch_labels.float().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_losses.append(loss.item())\n",
    "        iter_losses = np.array(iter_losses)\n",
    "        losses.append(np.mean(iter_losses))\n",
    "        \n",
    "        valid_mse = []\n",
    "        model.eval()\n",
    "\n",
    "        for valid_batch_stock_series, valid_batch_stock_aux, valid_batch_transcripts, valid_batch_labels in valid_iterator:\n",
    "            valid_batch_stock_aux = torch.reshape(valid_batch_stock_aux, (-1,1))\n",
    "            outputs = model(valid_batch_stock_series.float().cuda(), valid_batch_stock_aux.float().cuda(), valid_batch_transcripts.long().cuda())\n",
    "            valid_batch_labels = torch.reshape(valid_batch_labels, (-1,1))\n",
    "            loss = criterion(outputs.cuda(), valid_batch_labels.float().cuda())\n",
    "            valid_mse.append(loss.item())\n",
    "        valid_mse = np.mean(valid_mse)\n",
    "        print(f'Completed epoch {epoch}. Valid MSE: {valid_mse}')\n",
    "\n",
    "\n",
    "        if valid_mse < min_mse:\n",
    "            min_mse = valid_mse\n",
    "            delay = 0\n",
    "            torch.save(model, 'lg_model.ckpt')\n",
    "        else:\n",
    "            delay += 1\n",
    "        if delay > MAX_INC:\n",
    "            print('Stopping early')\n",
    "            break\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(train_dataset, valid_dataset, num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test):\n",
    "    batch_size = 64\n",
    "    test_iterator = get_lg_iterator(test, batch_size)\n",
    "    model.eval()\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    for batch_stock_series, batch_stock_aux, batch_transcripts, batch_labels in test_iterator:\n",
    "        batch_stock_aux = torch.reshape(batch_stock_aux, (-1,1))\n",
    "        outputs = model(batch_stock_series.float().cuda(), batch_stock_aux.float().cuda(), batch_transcripts.long().cuda())\n",
    "        batch_labels = torch.reshape(batch_labels, (-1,1))\n",
    "        loss = criterion(outputs.cuda(), batch_labels.float().cuda())\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_model = torch.load('lg_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaselineStockPredictor' object has no attribute 'linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-68b5b5d14e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-e91cfdd2033f>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, test)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_stock_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_stock_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_transcripts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_stock_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_stock_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_stock_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_stock_aux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_transcripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-377c6ef23cf8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_series, X_auxiliary, X_transcript)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mX_transcript_auxiliary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0man\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_auxiliary_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mbaseline_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_auxiliary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mbaseline_activated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-64f02294a71b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_series, X_auxiliary)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# recurrent_output = torch.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0maux_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecurrent_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_auxiliary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaselineStockPredictor' object has no attribute 'linear'"
     ]
    }
   ],
   "source": [
    "eval_model(lg_model, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lg_losses_180_epochs.json', 'w') as out:\n",
    "    json.dump(model[1], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
