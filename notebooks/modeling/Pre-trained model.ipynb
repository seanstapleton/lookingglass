{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/processed/splits/train/stock_data.json', 'r') as inp:\n",
    "    train_stock = json.load(inp)\n",
    "with open('../../data/processed/splits/valid/stock_data.json', 'r') as inp:\n",
    "    valid_stock = json.load(inp)\n",
    "with open('../../data/processed/splits/test/stock_data.json', 'r') as inp:\n",
    "    test_stock = json.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = Field(\n",
    "    sequential=False\n",
    ")\n",
    "TRANSCRIPT = Field(\n",
    "    sequential=True,\n",
    "    fix_length=11000,\n",
    "    lower=True\n",
    ")\n",
    "LABEL = Field(\n",
    "    sequential=False,\n",
    "    dtype=torch.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(data.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        examples = np.array(examples)\n",
    "        self.labels = examples[:,-1]\n",
    "        self.market_cap = examples[:,-2]\n",
    "        self.examples = np.array(examples[:,:-2].tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = torch.tensor(self.examples[index])\n",
    "        auxiliary = self.market_cap[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, auxiliary, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets():\n",
    "    train, valid, test = TabularDataset.splits(\n",
    "        path='../../data/processed/splits',\n",
    "        format='csv',\n",
    "        skip_header=True,\n",
    "        train='train/transcripts.csv',\n",
    "        validation='valid/transcripts.csv',\n",
    "        test='test/transcripts.csv',\n",
    "        fields=[('id', ID), ('transcript', TRANSCRIPT), ('post_high', LABEL)]\n",
    "    )\n",
    "    glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "    TRANSCRIPT.build_vocab(train, valid, test, vectors=glove)\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingGlassDataset(data.Dataset):\n",
    "    def __init__(self, stock_dataset, transcript_dataset):\n",
    "        self.stock_data = stock_dataset\n",
    "        self.transcript_data = transcript_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.stock_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        stocks, stocks_aux, label = self.stock_data[index]\n",
    "        transcript_example = self.transcript_data[index]\n",
    "        assert math.isclose(float(transcript_example.post_high), label)\n",
    "\n",
    "        transcript = torch.tensor(transcript_example.transcript)\n",
    "        return stocks, stocks_aux, transcript, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as inp:\n",
    "    VOCAB = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transcripts(train, valid, test):\n",
    "    for dataset in [train, valid, test]:\n",
    "        for example in dataset:\n",
    "            if len(example.transcript) > 11000:\n",
    "                example.transcript = example.transcript[:11000]\n",
    "            else:\n",
    "                remainder = 11000 - len(example.transcript)\n",
    "                example.transcript += ['<pad>']*remainder\n",
    "            example.transcript = list(map(lambda x: VOCAB.stoi[x], example.transcript))\n",
    "    return train, valid, test\n",
    "\n",
    "train_transcript, valid_transcript, test_transcript = preprocess_transcripts(*build_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_datasets = {\n",
    "    'train': StockDataset(train_stock),\n",
    "    'valid': StockDataset(valid_stock),\n",
    "    'test': StockDataset(test_stock)\n",
    "}\n",
    "\n",
    "train_dataset = LookingGlassDataset(stock_datasets['train'], train_transcript)\n",
    "valid_dataset = LookingGlassDataset(stock_datasets['valid'], valid_transcript)\n",
    "test_dataset = LookingGlassDataset(stock_datasets['test'], test_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineStockPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that will read in plain stock ticker values over time and decide whether to buy, sell, or hold at the current price.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_series_features=1, num_auxiliary_features=1, hidden_size=128, output_size=1):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            num_series_features: the size of the feature set for an individual\n",
    "                                 stock price example (e.g. if we include high,\n",
    "                                 low, average, num_series_features will equal 3\n",
    "            num_auxiliary_features: the number of auxiliary (not dependent on time)\n",
    "                                    features we are adding (e.g. if we include the 1yr\n",
    "                                    high and the market capitalization, num_auxiliary_features\n",
    "                                    would equal 2\n",
    "            output_size: the size of the outputted vector. For evaluation, we would use a\n",
    "                         size of 1 (stock price) or 3 (buy, sell, hold classification).\n",
    "                         For use in the looking glass model, we want an encoding so we might\n",
    "                         use a size of 128 to feed into the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.recurrent = nn.LSTM(\n",
    "            input_size=num_series_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        # concatenate LSTM output with auxiliary features\n",
    "        # output predicted price\n",
    "        self.linear1 = nn.Linear(hidden_size+num_auxiliary_features, output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the model\n",
    "        \"\"\"\n",
    "        for layer in [self.linear1]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, X_series, X_auxiliary):\n",
    "        \"\"\"\n",
    "        Moves the model through each layer\n",
    "        Parameters:\n",
    "            X_series: an [N, num_series_examples, num_series_features] size vector\n",
    "                      where N is the batch size, num_series_examples is how many stock prices\n",
    "                      we are providing per example (e.g. weekly for the last 3 months), and\n",
    "                      num_series_features is the same as described in __init__\n",
    "            X_auxiliary: an [N, num_auxiliary_features] vector\n",
    "        \"\"\"\n",
    "        recurrent_output,_ = self.recurrent(X_series)\n",
    "        recurrent_output = torch.mean(recurrent_output, 1)\n",
    "        # We might need this\n",
    "        # recurrent_output = torch.squeeze(1) \n",
    "        aux_combined = torch.cat([recurrent_output, X_auxiliary], dim=1)\n",
    "        output = self.linear1(aux_combined)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookingGlassPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that will use the Baseline predictor as well as earnings call information to decide whether to buy, sell, or hold at the current price\n",
    "    \"\"\"\n",
    "    def __init__(self, num_series_features=1, hidden_size=64, num_auxiliary_features=1, max_call_len=11000, baseline_weights=None):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        Attributes:\n",
    "            (see baseline.py for num_series_features and num_auxiliary_features)\n",
    "            max_call_len: maximum number of tokens allowed in an earnings call transcript.\n",
    "                          We will need to pad each earnings call to be this length (or truncate\n",
    "                          if the call is too long)\n",
    "            num_auxiliary_call_features: # non-transcript related features (e.g. if we\n",
    "                                         include sentiment, ambiguity score, and\n",
    "                                         confidence score, the num_auxiliary_call_features\n",
    "                                         would equal 3\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.baseline = BaselineStockPredictor(\n",
    "            num_series_features=num_series_features,\n",
    "            num_auxiliary_features=num_auxiliary_features,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=hidden_size\n",
    "        ).cuda()\n",
    "        \n",
    "        if baseline_weights is not None:\n",
    "            for (name, value),param in zip(baseline_weights.items(), self.baseline.parameters()):\n",
    "                if 'linear2' not in name:\n",
    "                    if param.data.shape == value.shape:\n",
    "                        param.data = value\n",
    "#                         param.requires_grad = False\n",
    "                    else:\n",
    "                        print(name, param.data.shape, value.shape)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(VOCAB.vectors, freeze=True)\n",
    "        self.recurrent = nn.LSTM(\n",
    "            input_size=50,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        self.rec_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.combined_linear = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.final_linear = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize the model weights\n",
    "        \"\"\"\n",
    "        self.baseline.init_weights()\n",
    "        for layer in [self.rec_linear, self.combined_linear, self.final_linear]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, X_series, X_auxiliary, X_transcript):\n",
    "        \"\"\"\n",
    "        Moves the model through each layer\n",
    "        Parameters:\n",
    "            (see baseline.py for X_series and X_auxiliary)\n",
    "            X_transcript: an [N, max_series_features, embedding_size] vector\n",
    "            X_transcript_auxiliary: an [N, num_auxiliary_features] vector\n",
    "        \"\"\"\n",
    "        baseline_output = self.baseline.forward(X_series, X_auxiliary)\n",
    "        baseline_activated = nn.functional.relu(baseline_output)\n",
    "\n",
    "        transcript_embeddings = self.embedding(X_transcript)\n",
    "        recurrent_output,_ = self.recurrent(transcript_embeddings)\n",
    "        recurrent_output = torch.mean(recurrent_output, 1)\n",
    "        \n",
    "#         aux_combined = torch.cat([recurrent_output, X_transcript_auxiliary], dim=1)\n",
    "        output = self.rec_linear(recurrent_output)\n",
    "        output_activated = nn.functional.relu(output)\n",
    "\n",
    "        stock_transcript_joint_layer = torch.cat([baseline_activated, output_activated], dim=1)\n",
    "        z1 = self.combined_linear(stock_transcript_joint_layer)\n",
    "        a1 = nn.functional.relu(z1)\n",
    "        \n",
    "        final_output = self.final_linear(a1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = torch.load('../../data/models/stock_6k.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lg_iterator(dataset, batch_size, train=True, shuffle=True):\n",
    "    iterator = data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=5)\n",
    "    return iterator\n",
    "    \n",
    "def train_model(train, valid, baseline, num_epochs=200, learning_rate=0.003, existing_model=None):\n",
    "    batch_size = 64\n",
    "    train_iterator = get_lg_iterator(train, batch_size)\n",
    "    valid_iterator = get_lg_iterator(valid, batch_size)\n",
    "    \n",
    "    model = None\n",
    "\n",
    "    if existing_model is None:\n",
    "        model = LookingGlassPredictor(num_series_features=2, hidden_size=64, baseline_weights=baseline.state_dict())\n",
    "    else:\n",
    "        model = existing_model\n",
    "    model = model.float()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    valid_scores = []\n",
    "    \n",
    "    min_mse = float('inf')\n",
    "    delay = 0\n",
    "    MAX_INC = 80\n",
    "    \n",
    "    for epoch in tqdm_notebook(range(num_epochs)):\n",
    "        model.train()\n",
    "        iter_losses = []\n",
    "        print('Starting epoch', epoch)\n",
    "        for batch_stock_series, batch_stock_aux, batch_transcripts, batch_labels in train_iterator:\n",
    "            batch_stock_aux = torch.reshape(batch_stock_aux, (-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_stock_series.float().cuda(), batch_stock_aux.float().cuda(), batch_transcripts.long().cuda())\n",
    "            batch_labels = torch.reshape(batch_labels, (-1,1))\n",
    "            loss = criterion(outputs.cuda(), batch_labels.float().cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_losses.append(loss.item())\n",
    "        iter_losses = np.array(iter_losses)\n",
    "        losses.append(np.mean(iter_losses))\n",
    "        \n",
    "        valid_mse = []\n",
    "        model.eval()\n",
    "\n",
    "        for valid_batch_stock_series, valid_batch_stock_aux, valid_batch_transcripts, valid_batch_labels in valid_iterator:\n",
    "            valid_batch_stock_aux = torch.reshape(valid_batch_stock_aux, (-1,1))\n",
    "            outputs = model(valid_batch_stock_series.float().cuda(), valid_batch_stock_aux.float().cuda(), valid_batch_transcripts.long().cuda())\n",
    "            valid_batch_labels = torch.reshape(valid_batch_labels, (-1,1))\n",
    "            loss = criterion(outputs.cuda(), valid_batch_labels.float().cuda())\n",
    "            valid_mse.append(loss.item())\n",
    "        valid_mse = np.mean(valid_mse)\n",
    "        print(f'Completed epoch {epoch}. Valid MSE: {valid_mse}')\n",
    "\n",
    "\n",
    "        if valid_mse < min_mse:\n",
    "            min_mse = valid_mse\n",
    "            delay = 0\n",
    "            torch.save(model, 'lg_model.ckpt')\n",
    "        else:\n",
    "            delay += 1\n",
    "        if delay > MAX_INC:\n",
    "            print('Stopping early')\n",
    "            break\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79da2c384ebc490cbbac991cc711dbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Completed epoch 0. Valid MSE: 3868.6046142578125\n",
      "Starting epoch 1\n",
      "Completed epoch 1. Valid MSE: 2517.458251953125\n",
      "Starting epoch 2\n",
      "Completed epoch 2. Valid MSE: 369.23743438720703\n",
      "Starting epoch 3\n",
      "Completed epoch 3. Valid MSE: 340.4572525024414\n",
      "Starting epoch 4\n",
      "Completed epoch 4. Valid MSE: 264.6954460144043\n",
      "Starting epoch 5\n",
      "Completed epoch 5. Valid MSE: 418.76341247558594\n",
      "Starting epoch 6\n",
      "Completed epoch 6. Valid MSE: 295.20481872558594\n",
      "Starting epoch 7\n",
      "Completed epoch 7. Valid MSE: 299.5159606933594\n",
      "Starting epoch 8\n",
      "Completed epoch 8. Valid MSE: 378.79632568359375\n",
      "Starting epoch 9\n",
      "Completed epoch 9. Valid MSE: 308.86622619628906\n",
      "Starting epoch 10\n",
      "Completed epoch 10. Valid MSE: 182.84267234802246\n",
      "Starting epoch 11\n",
      "Completed epoch 11. Valid MSE: 302.1366271972656\n",
      "Starting epoch 12\n",
      "Completed epoch 12. Valid MSE: 337.833984375\n",
      "Starting epoch 13\n",
      "Completed epoch 13. Valid MSE: 256.4351501464844\n",
      "Starting epoch 14\n",
      "Completed epoch 14. Valid MSE: 287.48252868652344\n",
      "Starting epoch 15\n",
      "Completed epoch 15. Valid MSE: 260.78844451904297\n",
      "Starting epoch 16\n",
      "Completed epoch 16. Valid MSE: 190.62418365478516\n",
      "Starting epoch 17\n",
      "Completed epoch 17. Valid MSE: 263.4235153198242\n",
      "Starting epoch 18\n",
      "Completed epoch 18. Valid MSE: 190.9177703857422\n",
      "Starting epoch 19\n",
      "Completed epoch 19. Valid MSE: 196.16558074951172\n",
      "Starting epoch 20\n",
      "Completed epoch 20. Valid MSE: 185.1005516052246\n",
      "Starting epoch 21\n",
      "Completed epoch 21. Valid MSE: 212.97608947753906\n",
      "Starting epoch 22\n",
      "Completed epoch 22. Valid MSE: 259.8885803222656\n",
      "Starting epoch 23\n",
      "Completed epoch 23. Valid MSE: 278.4840621948242\n",
      "Starting epoch 24\n",
      "Completed epoch 24. Valid MSE: 302.9325256347656\n",
      "Starting epoch 25\n",
      "Completed epoch 25. Valid MSE: 426.97601318359375\n",
      "Starting epoch 26\n",
      "Completed epoch 26. Valid MSE: 406.7386016845703\n",
      "Starting epoch 27\n",
      "Completed epoch 27. Valid MSE: 740.6282348632812\n",
      "Starting epoch 28\n",
      "Completed epoch 28. Valid MSE: 803.8354797363281\n",
      "Starting epoch 29\n",
      "Completed epoch 29. Valid MSE: 668.8649291992188\n",
      "Starting epoch 30\n",
      "Completed epoch 30. Valid MSE: 545.3536376953125\n",
      "Starting epoch 31\n",
      "Completed epoch 31. Valid MSE: 280.6449890136719\n",
      "Starting epoch 32\n",
      "Completed epoch 32. Valid MSE: 588.8139266967773\n",
      "Starting epoch 33\n",
      "Completed epoch 33. Valid MSE: 764.096923828125\n",
      "Starting epoch 34\n",
      "Completed epoch 34. Valid MSE: 182.85614776611328\n",
      "Starting epoch 35\n",
      "Completed epoch 35. Valid MSE: 212.8522186279297\n",
      "Starting epoch 36\n",
      "Completed epoch 36. Valid MSE: 713.742431640625\n",
      "Starting epoch 37\n",
      "Completed epoch 37. Valid MSE: 2763.06005859375\n",
      "Starting epoch 38\n",
      "Completed epoch 38. Valid MSE: 3985.54638671875\n",
      "Starting epoch 39\n",
      "Completed epoch 39. Valid MSE: 815.4005126953125\n",
      "Starting epoch 40\n",
      "Completed epoch 40. Valid MSE: 2666.628173828125\n",
      "Starting epoch 41\n",
      "Completed epoch 41. Valid MSE: 6151.288330078125\n",
      "Starting epoch 42\n",
      "Completed epoch 42. Valid MSE: 539.12890625\n",
      "Starting epoch 43\n",
      "Completed epoch 43. Valid MSE: 1756.6242065429688\n",
      "Starting epoch 44\n",
      "Completed epoch 44. Valid MSE: 443.7125549316406\n",
      "Starting epoch 45\n",
      "Completed epoch 45. Valid MSE: 1156.848388671875\n",
      "Starting epoch 46\n",
      "Completed epoch 46. Valid MSE: 448.28868103027344\n",
      "Starting epoch 47\n",
      "Completed epoch 47. Valid MSE: 284.91209411621094\n",
      "Starting epoch 48\n",
      "Completed epoch 48. Valid MSE: 826.419921875\n",
      "Starting epoch 49\n",
      "Completed epoch 49. Valid MSE: 513.6925430297852\n",
      "Starting epoch 50\n",
      "Completed epoch 50. Valid MSE: 619.6309204101562\n",
      "Starting epoch 51\n",
      "Completed epoch 51. Valid MSE: 859.7033081054688\n",
      "Starting epoch 52\n",
      "Completed epoch 52. Valid MSE: 238.76985931396484\n",
      "Starting epoch 53\n",
      "Completed epoch 53. Valid MSE: 659.7962646484375\n",
      "Starting epoch 54\n",
      "Completed epoch 54. Valid MSE: 166.30569458007812\n",
      "Starting epoch 55\n",
      "Completed epoch 55. Valid MSE: 578.4781494140625\n",
      "Starting epoch 56\n",
      "Completed epoch 56. Valid MSE: 222.5825653076172\n",
      "Starting epoch 57\n",
      "Completed epoch 57. Valid MSE: 1028.2337036132812\n",
      "Starting epoch 58\n",
      "Completed epoch 58. Valid MSE: 988.9183044433594\n",
      "Starting epoch 59\n",
      "Completed epoch 59. Valid MSE: 551.0008392333984\n",
      "Starting epoch 60\n",
      "Completed epoch 60. Valid MSE: 999.5454711914062\n",
      "Starting epoch 61\n",
      "Completed epoch 61. Valid MSE: 364.01354598999023\n",
      "Starting epoch 62\n",
      "Completed epoch 62. Valid MSE: 562.37646484375\n",
      "Starting epoch 63\n",
      "Completed epoch 63. Valid MSE: 375.50341796875\n",
      "Starting epoch 64\n",
      "Completed epoch 64. Valid MSE: 746.5764007568359\n",
      "Starting epoch 65\n",
      "Completed epoch 65. Valid MSE: 926.3337707519531\n",
      "Starting epoch 66\n",
      "Completed epoch 66. Valid MSE: 448.82456970214844\n",
      "Starting epoch 67\n",
      "Completed epoch 67. Valid MSE: 294.0682830810547\n",
      "Starting epoch 68\n",
      "Completed epoch 68. Valid MSE: 415.5824432373047\n",
      "Starting epoch 69\n",
      "Completed epoch 69. Valid MSE: 269.6506881713867\n",
      "Starting epoch 70\n",
      "Completed epoch 70. Valid MSE: 624.8889846801758\n",
      "Starting epoch 71\n",
      "Completed epoch 71. Valid MSE: 259.4602737426758\n",
      "Starting epoch 72\n",
      "Completed epoch 72. Valid MSE: 389.42645263671875\n",
      "Starting epoch 73\n",
      "Completed epoch 73. Valid MSE: 537.4011840820312\n",
      "Starting epoch 74\n",
      "Completed epoch 74. Valid MSE: 280.202392578125\n",
      "Starting epoch 75\n",
      "Completed epoch 75. Valid MSE: 303.9529800415039\n",
      "Starting epoch 76\n",
      "Completed epoch 76. Valid MSE: 351.5645294189453\n",
      "Starting epoch 77\n",
      "Completed epoch 77. Valid MSE: 573.2061309814453\n",
      "Starting epoch 78\n",
      "Completed epoch 78. Valid MSE: 304.9418716430664\n",
      "Starting epoch 79\n",
      "Completed epoch 79. Valid MSE: 249.3530502319336\n",
      "Starting epoch 80\n",
      "Completed epoch 80. Valid MSE: 358.9531707763672\n",
      "Starting epoch 81\n",
      "Completed epoch 81. Valid MSE: 458.7912063598633\n",
      "Starting epoch 82\n",
      "Completed epoch 82. Valid MSE: 196.0287857055664\n",
      "Starting epoch 83\n",
      "Completed epoch 83. Valid MSE: 205.5917510986328\n",
      "Starting epoch 84\n",
      "Completed epoch 84. Valid MSE: 515.0191040039062\n",
      "Starting epoch 85\n",
      "Completed epoch 85. Valid MSE: 315.8257141113281\n",
      "Starting epoch 86\n",
      "Completed epoch 86. Valid MSE: 294.7217483520508\n",
      "Starting epoch 87\n",
      "Completed epoch 87. Valid MSE: 539.7872467041016\n",
      "Starting epoch 88\n",
      "Completed epoch 88. Valid MSE: 444.25799560546875\n",
      "Starting epoch 89\n",
      "Completed epoch 89. Valid MSE: 405.5718765258789\n",
      "Starting epoch 90\n",
      "Completed epoch 90. Valid MSE: 5439.6033935546875\n",
      "Starting epoch 91\n",
      "Completed epoch 91. Valid MSE: 843.7147979736328\n",
      "Starting epoch 92\n",
      "Completed epoch 92. Valid MSE: 1583.79736328125\n",
      "Starting epoch 93\n",
      "Completed epoch 93. Valid MSE: 1382.6353149414062\n",
      "Starting epoch 94\n",
      "Completed epoch 94. Valid MSE: 333.0137176513672\n",
      "Starting epoch 95\n",
      "Completed epoch 95. Valid MSE: 1304.6228637695312\n",
      "Starting epoch 96\n",
      "Completed epoch 96. Valid MSE: 322.2826919555664\n",
      "Starting epoch 97\n",
      "Completed epoch 97. Valid MSE: 296.94591522216797\n",
      "Starting epoch 98\n",
      "Completed epoch 98. Valid MSE: 274.8439483642578\n",
      "Starting epoch 99\n",
      "Completed epoch 99. Valid MSE: 422.73106384277344\n",
      "Starting epoch 100\n",
      "Completed epoch 100. Valid MSE: 394.92325592041016\n",
      "Starting epoch 101\n",
      "Completed epoch 101. Valid MSE: 630.7156372070312\n",
      "Starting epoch 102\n",
      "Completed epoch 102. Valid MSE: 294.9166793823242\n",
      "Starting epoch 103\n",
      "Completed epoch 103. Valid MSE: 519.4791030883789\n",
      "Starting epoch 104\n",
      "Completed epoch 104. Valid MSE: 713.5643005371094\n",
      "Starting epoch 105\n",
      "Completed epoch 105. Valid MSE: 288.8996047973633\n",
      "Starting epoch 106\n",
      "Completed epoch 106. Valid MSE: 387.8576354980469\n",
      "Starting epoch 107\n",
      "Completed epoch 107. Valid MSE: 538.3449325561523\n",
      "Starting epoch 108\n",
      "Completed epoch 108. Valid MSE: 516.1200256347656\n",
      "Starting epoch 109\n",
      "Completed epoch 109. Valid MSE: 347.5242004394531\n",
      "Starting epoch 110\n",
      "Completed epoch 110. Valid MSE: 198.9687671661377\n",
      "Starting epoch 111\n",
      "Completed epoch 111. Valid MSE: 905.5921630859375\n",
      "Starting epoch 112\n",
      "Completed epoch 112. Valid MSE: 1127.2306518554688\n",
      "Starting epoch 113\n",
      "Completed epoch 113. Valid MSE: 652.4304046630859\n",
      "Starting epoch 114\n",
      "Completed epoch 114. Valid MSE: 1734.5762939453125\n",
      "Starting epoch 115\n",
      "Completed epoch 115. Valid MSE: 524.4175567626953\n",
      "Starting epoch 116\n",
      "Completed epoch 116. Valid MSE: 291.378662109375\n",
      "Starting epoch 117\n",
      "Completed epoch 117. Valid MSE: 577.3795166015625\n",
      "Starting epoch 118\n",
      "Completed epoch 118. Valid MSE: 701.50390625\n",
      "Starting epoch 119\n",
      "Completed epoch 119. Valid MSE: 653.6566162109375\n",
      "Starting epoch 120\n",
      "Completed epoch 120. Valid MSE: 772.2662658691406\n",
      "Starting epoch 121\n",
      "Completed epoch 121. Valid MSE: 525.1385650634766\n",
      "Starting epoch 122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 122. Valid MSE: 854.3472595214844\n",
      "Starting epoch 123\n",
      "Completed epoch 123. Valid MSE: 1357.2125854492188\n",
      "Starting epoch 124\n",
      "Completed epoch 124. Valid MSE: 577.3701019287109\n",
      "Starting epoch 125\n",
      "Completed epoch 125. Valid MSE: 297.90354919433594\n",
      "Starting epoch 126\n",
      "Completed epoch 126. Valid MSE: 356.0046081542969\n",
      "Starting epoch 127\n",
      "Completed epoch 127. Valid MSE: 431.24488830566406\n",
      "Starting epoch 128\n",
      "Completed epoch 128. Valid MSE: 545.0994110107422\n",
      "Starting epoch 129\n",
      "Completed epoch 129. Valid MSE: 250.21267700195312\n",
      "Starting epoch 130\n",
      "Completed epoch 130. Valid MSE: 209.8308334350586\n",
      "Starting epoch 131\n",
      "Completed epoch 131. Valid MSE: 205.9602813720703\n",
      "Starting epoch 132\n",
      "Completed epoch 132. Valid MSE: 179.7796220779419\n",
      "Starting epoch 133\n",
      "Completed epoch 133. Valid MSE: 236.35184478759766\n",
      "Starting epoch 134\n",
      "Completed epoch 134. Valid MSE: 285.6539993286133\n",
      "Starting epoch 135\n",
      "Completed epoch 135. Valid MSE: 252.4657745361328\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "lg_model2, losses1 = train_model(train_dataset, valid_dataset, baseline_model, num_epochs=200, existing_model = lg_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['recurrent.weight_ih_l0', 'recurrent.weight_hh_l0', 'recurrent.bias_ih_l0', 'recurrent.bias_hh_l0', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test):\n",
    "    batch_size = 128\n",
    "    test_iterator = get_lg_iterator(test, batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    for batch_stock_series, batch_stock_aux, batch_transcripts, batch_labels in test_iterator:\n",
    "        batch_stock_aux = torch.reshape(batch_stock_aux, (-1,1))\n",
    "        outputs = model(batch_stock_series.float().cuda(), batch_stock_aux.float().cuda(), batch_transcripts.long().cuda())\n",
    "        batch_labels = torch.reshape(batch_labels, (-1,1))\n",
    "        loss = criterion(outputs.cuda(), batch_labels.float().cuda())\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval_model_baseline(model, test):\n",
    "    batch_size = 128\n",
    "    test_iterator = get_lg_iterator(test, batch_size, shuffle=False)\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch_stock_series, batch_stock_aux, batch_transcripts, batch_labels in test_iterator:\n",
    "        batch_stock_aux = torch.reshape(batch_stock_aux, (-1,1))\n",
    "        outputs = model(batch_stock_series.float().cuda(), batch_stock_aux.float().cuda())\n",
    "        batch_labels = torch.reshape(batch_labels, (-1,1))\n",
    "        loss = criterion(outputs, batch_labels.float().cuda())\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621.9088134765625"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(lg_model2, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 136)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(losses), len(losses1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[57791.01806640625,\n",
       " 41765.090576171875,\n",
       " 49242.14697265625,\n",
       " 39555.488037109375,\n",
       " 38901.3603515625,\n",
       " 38614.108154296875,\n",
       " 46313.680013020836,\n",
       " 39562.42635091146,\n",
       " 38635.38728841146,\n",
       " 45981.48543294271,\n",
       " 38271.709635416664,\n",
       " 38408.105631510414,\n",
       " 38231.034423828125,\n",
       " 37923.4111328125,\n",
       " 38867.24694824219,\n",
       " 38267.087076822914,\n",
       " 37828.23779296875,\n",
       " 37453.446044921875,\n",
       " 46381.73478190104,\n",
       " 45597.123779296875,\n",
       " 37574.11551920573,\n",
       " 37298.36083984375,\n",
       " 37042.51021321615,\n",
       " 37398.85404459635,\n",
       " 37239.03377278646,\n",
       " 37695.6650390625,\n",
       " 38167.640625,\n",
       " 36792.414123535156,\n",
       " 44308.742431640625,\n",
       " 36920.99650065104,\n",
       " 45672.737467447914,\n",
       " 37451.074055989586,\n",
       " 37104.071614583336,\n",
       " 43765.85359700521,\n",
       " 36727.4472249349,\n",
       " 47291.23392740885,\n",
       " 36761.545654296875,\n",
       " 44285.685384114586,\n",
       " 36750.63415527344,\n",
       " 39795.615559895836,\n",
       " 37035.04890950521,\n",
       " 37462.063802083336,\n",
       " 38940.532958984375,\n",
       " 36781.2431640625,\n",
       " 36941.57413736979,\n",
       " 36478.21048990885,\n",
       " 36880.685221354164,\n",
       " 36534.810709635414,\n",
       " 36852.89786783854,\n",
       " 39132.13582356771,\n",
       " 36419.460123697914,\n",
       " 36629.74271647135,\n",
       " 36897.46329752604,\n",
       " 36896.707356770836,\n",
       " 36456.772216796875,\n",
       " 36506.82604980469,\n",
       " 36424.23917643229,\n",
       " 37250.35164388021,\n",
       " 44452.340169270836,\n",
       " 36370.76721191406,\n",
       " 37661.84802246094,\n",
       " 36304.88757324219,\n",
       " 37483.75284830729,\n",
       " 46442.616943359375,\n",
       " 37821.928385416664,\n",
       " 36541.6650390625,\n",
       " 36963.00850423177,\n",
       " 36849.297607421875,\n",
       " 38743.960774739586,\n",
       " 36946.94083658854,\n",
       " 37209.043294270836,\n",
       " 36740.10965983073,\n",
       " 38216.694498697914,\n",
       " 44179.646728515625,\n",
       " 36670.471028645836,\n",
       " 36583.07767740885,\n",
       " 36255.74405924479,\n",
       " 36059.69580078125,\n",
       " 44648.431396484375,\n",
       " 46435.11865234375,\n",
       " 36835.32625325521,\n",
       " 36412.136881510414,\n",
       " 36978.263427734375,\n",
       " 36135.89021809896,\n",
       " 43848.438639322914,\n",
       " 36313.35900878906,\n",
       " 36223.33158365885,\n",
       " 36568.627604166664,\n",
       " 8371.316994984945,\n",
       " 4630.3103586832685,\n",
       " 2236.456128438314,\n",
       " 812.0228907267252,\n",
       " 644.4454345703125,\n",
       " 471.20050557454425,\n",
       " 567.642588297526,\n",
       " 435.67226791381836,\n",
       " 413.44451904296875,\n",
       " 435.1610361735026,\n",
       " 617.580763498942,\n",
       " 436.3593266805013,\n",
       " 367.6069971720378,\n",
       " 306.9898312886556,\n",
       " 392.2425651550293,\n",
       " 424.4247175852458,\n",
       " 330.34051450093585,\n",
       " 301.5306650797526,\n",
       " 271.43620745340985,\n",
       " 230.9335028330485,\n",
       " 258.2220567067464,\n",
       " 394.3350601196289,\n",
       " 267.6812286376953,\n",
       " 439.6445089975993,\n",
       " 330.48330307006836,\n",
       " 257.99816195170087,\n",
       " 371.30465571085614,\n",
       " 816.2593765258789,\n",
       " 657.182191212972,\n",
       " 942.5675290425619,\n",
       " 788.0590629577637,\n",
       " 525.9144643147787,\n",
       " 538.9575169881185,\n",
       " 635.9285933176676,\n",
       " 607.4457626342773,\n",
       " 450.544002532959,\n",
       " 852.7571830749512,\n",
       " 1555.9739061991374,\n",
       " 1989.652328491211,\n",
       " 8131.3130289713545,\n",
       " 6051.602249145508,\n",
       " 5894.687327067058,\n",
       " 8819.812866210938,\n",
       " 2428.043904622396,\n",
       " 1885.8471984863281,\n",
       " 1457.190823872884,\n",
       " 738.9751014709473,\n",
       " 979.7656555175781,\n",
       " 1101.0139617919922,\n",
       " 1322.0819358825684,\n",
       " 825.6849466959635,\n",
       " 1090.7046915690105,\n",
       " 1920.6958923339844,\n",
       " 1224.4414647420247,\n",
       " 645.9648742675781,\n",
       " 553.1860771179199,\n",
       " 539.0632851918539,\n",
       " 808.5464401245117,\n",
       " 1216.8373934427898,\n",
       " 514.9011898040771,\n",
       " 452.6923713684082,\n",
       " 484.15270105997723,\n",
       " 578.6226272583008,\n",
       " 492.42902692159015,\n",
       " 607.79035059611,\n",
       " 1001.2191619873047,\n",
       " 661.1875813802084,\n",
       " 432.6383120218913,\n",
       " 391.84153747558594,\n",
       " 321.12455495198566,\n",
       " 301.6016305287679,\n",
       " 290.5506985982259,\n",
       " 248.37454414367676,\n",
       " 275.3153403600057,\n",
       " 222.5035088857015,\n",
       " 266.92929045359296,\n",
       " 346.26378504435223,\n",
       " 388.82577959696454,\n",
       " 453.7137101491292,\n",
       " 260.9142201741536,\n",
       " 477.80237102508545,\n",
       " 465.96204630533856,\n",
       " 294.4024772644043,\n",
       " 300.88361676534015,\n",
       " 246.5957794189453,\n",
       " 343.86266708374023,\n",
       " 484.44108327229816,\n",
       " 604.8508135477701,\n",
       " 425.16653362909955,\n",
       " 449.9263000488281,\n",
       " 1064.7178115844727,\n",
       " 2231.944035847982,\n",
       " 1070.9722188313801,\n",
       " 3188.2920735677085,\n",
       " 2098.730094909668,\n",
       " 1000.3608678181967,\n",
       " 1315.0551923116047,\n",
       " 940.074462890625,\n",
       " 853.7659721374512,\n",
       " 366.51391410827637,\n",
       " 321.293020884196,\n",
       " 462.4021581013997,\n",
       " 519.4529418945312,\n",
       " 434.023198445638,\n",
       " 616.1051877339681,\n",
       " 490.8381236394246,\n",
       " 269.04436111450195,\n",
       " 404.3676840464274,\n",
       " 355.43467966715497,\n",
       " 279.64607429504395,\n",
       " 481.3429463704427,\n",
       " 487.27054659525555,\n",
       " 1713.559232076009,\n",
       " 797.474547068278,\n",
       " 1333.2628479003906,\n",
       " 744.7651532491049,\n",
       " 459.30896250406903,\n",
       " 439.29734738667804,\n",
       " 1172.5298411051433,\n",
       " 669.7077115376791,\n",
       " 686.8339188893636,\n",
       " 730.137555440267,\n",
       " 448.757355372111,\n",
       " 841.8335990905762,\n",
       " 684.9308751424154,\n",
       " 422.95241292317706,\n",
       " 394.46071656545,\n",
       " 902.591168085734,\n",
       " 730.7367045084635,\n",
       " 651.1796518961588,\n",
       " 377.04051844278973,\n",
       " 312.9867362976074,\n",
       " 271.0568141937256,\n",
       " 226.3645559946696,\n",
       " 218.2479372024536,\n",
       " 219.73759746551514]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses + losses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
